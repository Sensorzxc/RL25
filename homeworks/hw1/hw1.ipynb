{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\skfim\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\~~mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\skfim\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\~~mpy'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"git+https://github.com/LucasAlegre/sumo-rl.git\" stable-baselines3[extra]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import sumo_rl\n",
    "\n",
    "root = os.path.abspath(\"sumo-rl\")\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumo_rl.agents import QLAgent\n",
    "from sumo_rl.exploration import EpsilonGreedy\n",
    "\n",
    "\n",
    "def train_baseline_qlearning(episodes=4, runs=1, alpha=0.1, gamma=0.99, epsilon=0.05, epsilon_min=0.005, decay=1.0, num_seconds=80000, delta_time=5, min_green=5):\n",
    "    env = sumo_rl.SumoEnvironment(\n",
    "        net_file=\"sumo-rl/sumo_rl/nets/4x4-Lucas/4x4.net.xml\",\n",
    "        route_file=\"sumo-rl/sumo_rl/nets/4x4-Lucas/4x4c1c2c1c2.rou.xml\",\n",
    "        use_gui=False,\n",
    "        num_seconds=num_seconds,\n",
    "        min_green=min_green,\n",
    "        delta_time=delta_time,\n",
    "    )\n",
    "    logs = []\n",
    "    for run in range(1, runs + 1):\n",
    "        initial_states = env.reset()\n",
    "        agents = {\n",
    "            ts: QLAgent(\n",
    "                starting_state=env.encode(initial_states[ts], ts),\n",
    "                state_space=env.observation_space,\n",
    "                action_space=env.action_space,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "                exploration_strategy=EpsilonGreedy(initial_epsilon=epsilon, min_epsilon=epsilon_min, decay=decay),\n",
    "            )\n",
    "            for ts in env.ts_ids\n",
    "        }\n",
    "        for ep in range(1, episodes + 1):\n",
    "            if ep != 1:\n",
    "                initial_states = env.reset()\n",
    "                for ts in initial_states.keys():\n",
    "                    agents[ts].state = env.encode(initial_states[ts], ts)\n",
    "            done = {\"__all__\": False}\n",
    "            total_reward = 0.0\n",
    "            while not done[\"__all__\"]:\n",
    "                actions = {ts: agents[ts].act() for ts in agents.keys()}\n",
    "                s, r, done, info = env.step(action=actions)\n",
    "                for agent_id in s.keys():\n",
    "                    agents[agent_id].learn(next_state=env.encode(s[agent_id], agent_id), reward=r[agent_id])\n",
    "                    total_reward += r[agent_id]\n",
    "            env.save_csv(f\"outputs/4x4/baseline_run{run}\", ep)\n",
    "            logs.append({\"run\": run, \"episode\": ep, \"reward\": total_reward})\n",
    "    env.close()\n",
    "    df = pd.DataFrame(logs)\n",
    "    df[\"reward_mean\"] = df[\"reward\"].mean()\n",
    "    df[\"reward_std\"] = df[\"reward\"].std()\n",
    "    df[\"reward_min\"] = df[\"reward\"].min()\n",
    "    df[\"reward_max\"] = df[\"reward\"].max()\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    df.to_csv(\"outputs/ql_baseline.csv\", index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedQLearner:\n",
    "    def __init__(self, action_space, alpha=0.5, gamma=0.99, epsilon_start=1.0, epsilon_final=0.05, epsilon_decay_steps=3000, alpha_decay=0.999, alpha_min=0.05):\n",
    "        self.n_actions = action_space.n\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.alpha_min = alpha_min\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_final = epsilon_final\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.steps = 0\n",
    "        self.q = {}\n",
    "\n",
    "    def _key(self, obs):\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            return tuple(obs.tolist())\n",
    "        if isinstance(obs, (list, tuple)):\n",
    "            return tuple(obs)\n",
    "        return obs\n",
    "\n",
    "    def _eps(self):\n",
    "        frac = min(1.0, self.steps / max(1, self.epsilon_decay_steps))\n",
    "        return self.epsilon_start + frac * (self.epsilon_final - self.epsilon_start)\n",
    "\n",
    "    def act(self, obs):\n",
    "        self.steps += 1\n",
    "        key = self._key(obs)\n",
    "        if key not in self.q:\n",
    "            self.q[key] = np.zeros(self.n_actions, dtype=np.float32)\n",
    "        if random.random() < self._eps():\n",
    "            return random.randrange(self.n_actions)\n",
    "        return int(np.argmax(self.q[key]))\n",
    "\n",
    "    def learn(self, obs, action, reward, next_obs, done):\n",
    "        key = self._key(obs)\n",
    "        next_key = self._key(next_obs)\n",
    "        if key not in self.q:\n",
    "            self.q[key] = np.zeros(self.n_actions, dtype=np.float32)\n",
    "        if next_key not in self.q:\n",
    "            self.q[next_key] = np.zeros(self.n_actions, dtype=np.float32)\n",
    "        r = float(np.clip(reward, -1.0, 1.0))\n",
    "        target = r\n",
    "        if not done:\n",
    "            target += self.gamma * np.max(self.q[next_key])\n",
    "        td = target - self.q[key][action]\n",
    "        self.q[key][action] += self.alpha * td\n",
    "        self.alpha = max(self.alpha * self.alpha_decay, self.alpha_min)\n",
    "\n",
    "\n",
    "def train_optimized_qlearning(episodes=10, num_seconds=600, delta_time=15):\n",
    "    env = sumo_rl.parallel_env(\n",
    "        net_file=\"sumo-rl/sumo_rl/nets/4x4-Lucas/4x4.net.xml\",\n",
    "        route_file=\"sumo-rl/sumo_rl/nets/4x4-Lucas/4x4c1c2c1c2.rou.xml\",\n",
    "        use_gui=False,\n",
    "        num_seconds=num_seconds,\n",
    "        delta_time=delta_time,\n",
    "    )\n",
    "    obs, _ = env.reset()\n",
    "    agents = {aid: OptimizedQLearner(env.action_space(aid)) for aid in env.possible_agents}\n",
    "    log = []\n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0.0\n",
    "        while env.agents:\n",
    "            actions = {aid: agents[aid].act(obs[aid]) for aid in env.agents if aid in obs}\n",
    "            next_obs, rewards, terms, truncs, _ = env.step(actions)\n",
    "            for aid in env.agents:\n",
    "                if aid in obs and aid in next_obs:\n",
    "                    agents[aid].learn(obs[aid], actions[aid], rewards.get(aid, 0.0), next_obs[aid], terms.get(aid, False))\n",
    "                    total_reward += rewards.get(aid, 0.0)\n",
    "            obs = next_obs\n",
    "        log.append({\"episode\": ep + 1, \"reward\": total_reward, \"epsilon\": agents[list(agents.keys())[0]]._eps()})\n",
    "    env.close()\n",
    "    df = pd.DataFrame(log)\n",
    "    df[\"reward_mean\"] = df[\"reward\"].mean()\n",
    "    df[\"reward_std\"] = df[\"reward\"].std()\n",
    "    df[\"reward_min\"] = df[\"reward\"].min()\n",
    "    df[\"reward_max\"] = df[\"reward\"].max()\n",
    "    df[\"q_table_size\"] = sum(len(agent.q) for agent in agents.values())\n",
    "    df[\"alpha_final\"] = agents[list(agents.keys())[0]].alpha\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    df.to_csv(\"outputs/ql_optimized.csv\", index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m basic_df = \u001b[43mtrain_baseline_qlearning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta_time\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m opt_df = train_optimized_qlearning(episodes=\u001b[32m5\u001b[39m)\n\u001b[32m      4\u001b[39m summary = pd.DataFrame({\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mvariant\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mbaseline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33moptimized\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmean_reward\u001b[39m\u001b[33m\"\u001b[39m: [basic_df[\u001b[33m\"\u001b[39m\u001b[33mreward\u001b[39m\u001b[33m\"\u001b[39m].mean(), opt_df[\u001b[33m\"\u001b[39m\u001b[33mreward\u001b[39m\u001b[33m\"\u001b[39m].mean()],\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mepisodes\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[38;5;28mlen\u001b[39m(basic_df), \u001b[38;5;28mlen\u001b[39m(opt_df)],\n\u001b[32m     12\u001b[39m })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mtrain_baseline_qlearning\u001b[39m\u001b[34m(episodes, runs, alpha, gamma, epsilon, epsilon_min, decay, num_seconds, delta_time, min_green)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done[\u001b[33m\"\u001b[39m\u001b[33m__all__\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     39\u001b[39m     actions = {ts: agents[ts].act() \u001b[38;5;28;01mfor\u001b[39;00m ts \u001b[38;5;129;01min\u001b[39;00m agents.keys()}\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     s, r, done, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m agent_id \u001b[38;5;129;01min\u001b[39;00m s.keys():\n\u001b[32m     42\u001b[39m         agents[agent_id].learn(next_state=env.encode(s[agent_id], agent_id), reward=r[agent_id])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sumo_rl\\environment\\env.py:308\u001b[39m, in \u001b[36mSumoEnvironment.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    306\u001b[39m terminated = \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# there are no 'terminal' states in this environment\u001b[39;00m\n\u001b[32m    307\u001b[39m truncated = dones[\u001b[33m\"\u001b[39m\u001b[33m__all__\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# episode ends when sim_step >= max_steps\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.single_agent:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m observations[\u001b[38;5;28mself\u001b[39m.ts_ids[\u001b[32m0\u001b[39m]], rewards[\u001b[38;5;28mself\u001b[39m.ts_ids[\u001b[32m0\u001b[39m]], terminated, truncated, info\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sumo_rl\\environment\\env.py:349\u001b[39m, in \u001b[36mSumoEnvironment._compute_info\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    347\u001b[39m     info.update(\u001b[38;5;28mself\u001b[39m._get_system_info())\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.add_per_agent_info:\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m     info.update(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_per_agent_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    350\u001b[39m \u001b[38;5;28mself\u001b[39m.metrics.append(info.copy())\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m info\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sumo_rl\\environment\\env.py:447\u001b[39m, in \u001b[36mSumoEnvironment._get_per_agent_info\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    443\u001b[39m stopped = [\u001b[38;5;28mself\u001b[39m.traffic_signals[ts].get_total_queued() \u001b[38;5;28;01mfor\u001b[39;00m ts \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ts_ids]\n\u001b[32m    444\u001b[39m accumulated_waiting_time = [\n\u001b[32m    445\u001b[39m     \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m.traffic_signals[ts].get_accumulated_waiting_time_per_lane()) \u001b[38;5;28;01mfor\u001b[39;00m ts \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ts_ids\n\u001b[32m    446\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m average_speed = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraffic_signals\u001b[49m\u001b[43m[\u001b[49m\u001b[43mts\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_average_speed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mts_ids\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    448\u001b[39m info = {}\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, ts \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.ts_ids):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sumo_rl\\environment\\env.py:447\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    443\u001b[39m stopped = [\u001b[38;5;28mself\u001b[39m.traffic_signals[ts].get_total_queued() \u001b[38;5;28;01mfor\u001b[39;00m ts \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ts_ids]\n\u001b[32m    444\u001b[39m accumulated_waiting_time = [\n\u001b[32m    445\u001b[39m     \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m.traffic_signals[ts].get_accumulated_waiting_time_per_lane()) \u001b[38;5;28;01mfor\u001b[39;00m ts \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ts_ids\n\u001b[32m    446\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m average_speed = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraffic_signals\u001b[49m\u001b[43m[\u001b[49m\u001b[43mts\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_average_speed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ts \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ts_ids]\n\u001b[32m    448\u001b[39m info = {}\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, ts \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.ts_ids):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sumo_rl\\environment\\traffic_signal.py:276\u001b[39m, in \u001b[36mTrafficSignal.get_average_speed\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m1.0\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m vehs:\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     avg_speed += \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msumo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvehicle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetSpeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m / \u001b[38;5;28mself\u001b[39m.sumo.vehicle.getAllowedSpeed(v)\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m avg_speed / \u001b[38;5;28mlen\u001b[39m(vehs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\_vehicle.py:282\u001b[39m, in \u001b[36mVehicleDomain.getSpeed\u001b[39m\u001b[34m(self, vehID)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgetSpeed\u001b[39m(\u001b[38;5;28mself\u001b[39m, vehID):\n\u001b[32m    278\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"getSpeed(string) -> double\u001b[39;00m\n\u001b[32m    279\u001b[39m \n\u001b[32m    280\u001b[39m \u001b[33;03m    Returns the (longitudinal) speed in m/s of the named vehicle within the last step.\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getUniversal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVAR_SPEED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvehID\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\domain.py:152\u001b[39m, in \u001b[36mDomain._getUniversal\u001b[39m\u001b[34m(self, varID, objectID, format, *values)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._deprecatedFor:\n\u001b[32m    151\u001b[39m     warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mThe domain \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m is deprecated, use \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m\"\u001b[39m % (\u001b[38;5;28mself\u001b[39m._name, \u001b[38;5;28mself\u001b[39m._deprecatedFor))\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _parse(\u001b[38;5;28mself\u001b[39m._retValFunc, varID, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\domain.py:157\u001b[39m, in \u001b[36mDomain._getCmd\u001b[39m\u001b[34m(self, varID, objID, format, *values)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FatalTraCIError(\u001b[33m\"\u001b[39m\u001b[33mNot connected.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmdGetID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m r.readLength()\n\u001b[32m    159\u001b[39m response, retVarID = r.read(\u001b[33m\"\u001b[39m\u001b[33m!BB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:232\u001b[39m, in \u001b[36mConnection._sendCmd\u001b[39m\u001b[34m(self, cmdID, varID, objID, format, *values)\u001b[39m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28mself\u001b[39m._string += struct.pack(\u001b[33m\"\u001b[39m\u001b[33m!i\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) + objID\n\u001b[32m    231\u001b[39m \u001b[38;5;28mself\u001b[39m._string += packed\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:130\u001b[39m, in \u001b[36mConnection._sendExact\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msending\u001b[39m\u001b[33m\"\u001b[39m, Storage(length + \u001b[38;5;28mself\u001b[39m._string).getDebugString())\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m result = \u001b[38;5;28mself\u001b[39m._recvExact()\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "basic_df = train_baseline_qlearning(episodes=5, num_seconds=600, delta_time=10)\n",
    "opt_df = train_optimized_qlearning(episodes=5)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"variant\": [\"baseline\", \"optimized\"],\n",
    "    \"mean_reward\": [basic_df[\"reward\"].mean(), opt_df[\"reward\"].mean()],\n",
    "    \"std_reward\": [basic_df[\"reward\"].std(), opt_df[\"reward\"].std()],\n",
    "    \"min_reward\": [basic_df[\"reward\"].min(), opt_df[\"reward\"].min()],\n",
    "    \"max_reward\": [basic_df[\"reward\"].max(), opt_df[\"reward\"].max()],\n",
    "    \"episodes\": [len(basic_df), len(opt_df)],\n",
    "})\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    def add(self, s, a, r, sn, d):\n",
    "        self.buffer.append((s, a, r, sn, d))\n",
    "    def sample(self, batch):\n",
    "        batch = random.sample(self.buffer, batch)\n",
    "        s, a, r, sn, d = zip(*batch)\n",
    "        return np.array(s), np.array(a), np.array(r, dtype=np.float32), np.array(sn), np.array(d, dtype=np.float32)\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden=(128, 128)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = obs_dim\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(last, h), nn.ReLU()]\n",
    "            last = h\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.value = nn.Linear(last, 1)\n",
    "        self.advantage = nn.Linear(last, act_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.features(x)\n",
    "        v = self.value(feat)\n",
    "        a = self.advantage(feat)\n",
    "        return v + (a - a.mean(dim=1, keepdim=True))\n",
    "\n",
    "class DQNTrafficAgent:\n",
    "    def __init__(self, env, hidden=(128, 128), lr=1e-3, gamma=0.99, epsilon_start=1.0, epsilon_final=0.05, exploration_fraction=0.5, buffer_size=50000, batch_size=128, train_freq=4, target_update=1000, learning_starts=10000, max_grad_norm=10.0, use_double_dqn=True, n_step=1, device=None):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_final = epsilon_final\n",
    "        self.exploration_fraction = exploration_fraction\n",
    "        self.batch_size = batch_size\n",
    "        self.train_freq = train_freq\n",
    "        self.target_update = target_update\n",
    "        self.learning_starts = learning_starts\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.use_double_dqn = use_double_dqn\n",
    "        self.n_step = n_step\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        act_dim = env.action_space.n\n",
    "        self.q = DuelingDQN(obs_dim, act_dim, hidden).to(self.device)\n",
    "        self.target = DuelingDQN(obs_dim, act_dim, hidden).to(self.device)\n",
    "        self.target.load_state_dict(self.q.state_dict())\n",
    "        self.opt = optim.Adam(self.q.parameters(), lr=lr)\n",
    "        self.buffer = ReplayBuffer(buffer_size)\n",
    "        self.n_step_buffer = deque(maxlen=n_step)\n",
    "        self.steps = 0\n",
    "\n",
    "    def _epsilon(self, total_steps):\n",
    "        frac = min(1.0, self.steps / max(1, int(total_steps * self.exploration_fraction)))\n",
    "        return self.epsilon_start + frac * (self.epsilon_final - self.epsilon_start)\n",
    "\n",
    "    def act(self, obs, total_steps):\n",
    "        eps = self._epsilon(total_steps)\n",
    "        if random.random() < eps:\n",
    "            return self.env.action_space.sample()\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            qv = self.q(obs_t)\n",
    "        return int(torch.argmax(qv, dim=1).item())\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "        s, a, r, sn, d = self.buffer.sample(self.batch_size)\n",
    "        s = torch.as_tensor(s, dtype=torch.float32, device=self.device)\n",
    "        a = torch.as_tensor(a, dtype=torch.int64, device=self.device).unsqueeze(1)\n",
    "        r = torch.as_tensor(r, dtype=torch.float32, device=self.device)\n",
    "        sn = torch.as_tensor(sn, dtype=torch.float32, device=self.device)\n",
    "        d = torch.as_tensor(d, dtype=torch.float32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            if self.use_double_dqn:\n",
    "                next_actions = self.q(sn).argmax(1, keepdim=True)\n",
    "                q_next = self.target(sn).gather(1, next_actions).squeeze(1)\n",
    "            else:\n",
    "                q_next = self.target(sn).max(1)[0]\n",
    "            target = r + (self.gamma ** self.n_step) * q_next * (1.0 - d)\n",
    "        q_sa = self.q(s).gather(1, a).squeeze(1)\n",
    "        loss = nn.functional.smooth_l1_loss(q_sa, target)\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q.parameters(), self.max_grad_norm)\n",
    "        self.opt.step()\n",
    "\n",
    "    def learn(self, total_steps=20000):\n",
    "        obs, _ = self.env.reset()\n",
    "        for _ in range(total_steps):\n",
    "            self.steps += 1\n",
    "            action = self.act(obs, total_steps)\n",
    "            next_obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            reward_scaled = np.clip(reward / 100.0, -10, 10)\n",
    "            self.n_step_buffer.append((obs, action, reward_scaled, next_obs, done))\n",
    "            if len(self.n_step_buffer) == self.n_step:\n",
    "                n_reward = sum([self.gamma**i * self.n_step_buffer[i][2] for i in range(len(self.n_step_buffer))])\n",
    "                s0, a0 = self.n_step_buffer[0][0], self.n_step_buffer[0][1]\n",
    "                sn, dn = self.n_step_buffer[-1][3], self.n_step_buffer[-1][4]\n",
    "                self.buffer.add(s0, a0, n_reward, sn, dn)\n",
    "            if done:\n",
    "                while len(self.n_step_buffer) > 0:\n",
    "                    n_reward = sum([self.gamma**i * self.n_step_buffer[i][2] for i in range(len(self.n_step_buffer))])\n",
    "                    s0, a0 = self.n_step_buffer[0][0], self.n_step_buffer[0][1]\n",
    "                    sn, dn = self.n_step_buffer[-1][3], self.n_step_buffer[-1][4]\n",
    "                    self.buffer.add(s0, a0, n_reward, sn, dn)\n",
    "                    self.n_step_buffer.popleft()\n",
    "            if self.steps >= self.learning_starts and self.steps % self.train_freq == 0:\n",
    "                self.train_step()\n",
    "            if self.steps % self.target_update == 0:\n",
    "                self.target.load_state_dict(self.q.state_dict())\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                obs, _ = self.env.reset()\n",
    "        return self.q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom DQN training completed in 25.46 minutes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25.455737221240998"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_custom_dqn(total_steps=15000):\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    base = os.getcwd()\n",
    "    env = sumo_rl.SumoEnvironment(\n",
    "        net_file=os.path.join(base, \"big-intersection\", \"big-intersection.net.xml\"),\n",
    "        route_file=os.path.join(base, \"big-intersection\", \"routes.rou.xml\"),\n",
    "        single_agent=True,\n",
    "        out_csv_name=\"outputs/big-intersection/custom-dqn\",\n",
    "        use_gui=False,\n",
    "        num_seconds=600,\n",
    "        delta_time=15,\n",
    "        yellow_time=4,\n",
    "        min_green=5,\n",
    "        max_green=60,\n",
    "    )\n",
    "    agent = DQNTrafficAgent(env, exploration_fraction=0.5, learning_starts=3000)\n",
    "    agent.learn(total_steps)\n",
    "    env.close()\n",
    "    elapsed_min = (time.time() - start) / 60.0\n",
    "    print(f\"Custom DQN training completed in {elapsed_min:.2f} minutes\")\n",
    "    return elapsed_min\n",
    "\n",
    "train_custom_dqn(total_steps=15000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -180     |\n",
      "|    exploration_rate | 0.98     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 160      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -213     |\n",
      "|    exploration_rate | 0.959    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 320      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -219     |\n",
      "|    exploration_rate | 0.939    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 480      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -224     |\n",
      "|    exploration_rate | 0.919    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 640      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -230     |\n",
      "|    exploration_rate | 0.899    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 96       |\n",
      "|    total_timesteps  | 800      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -228     |\n",
      "|    exploration_rate | 0.878    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 117      |\n",
      "|    total_timesteps  | 960      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -228     |\n",
      "|    exploration_rate | 0.858    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 137      |\n",
      "|    total_timesteps  | 1120     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -216     |\n",
      "|    exploration_rate | 0.838    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 156      |\n",
      "|    total_timesteps  | 1280     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -217     |\n",
      "|    exploration_rate | 0.818    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 178      |\n",
      "|    total_timesteps  | 1440     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -226     |\n",
      "|    exploration_rate | 0.797    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 199      |\n",
      "|    total_timesteps  | 1600     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -221     |\n",
      "|    exploration_rate | 0.777    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 218      |\n",
      "|    total_timesteps  | 1760     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -217     |\n",
      "|    exploration_rate | 0.757    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 237      |\n",
      "|    total_timesteps  | 1920     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -215     |\n",
      "|    exploration_rate | 0.737    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 256      |\n",
      "|    total_timesteps  | 2080     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -209     |\n",
      "|    exploration_rate | 0.716    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 275      |\n",
      "|    total_timesteps  | 2240     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -208     |\n",
      "|    exploration_rate | 0.696    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 294      |\n",
      "|    total_timesteps  | 2400     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -205     |\n",
      "|    exploration_rate | 0.676    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 313      |\n",
      "|    total_timesteps  | 2560     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -205     |\n",
      "|    exploration_rate | 0.655    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 333      |\n",
      "|    total_timesteps  | 2720     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -206     |\n",
      "|    exploration_rate | 0.635    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 353      |\n",
      "|    total_timesteps  | 2880     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -207     |\n",
      "|    exploration_rate | 0.615    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 372      |\n",
      "|    total_timesteps  | 3040     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.98     |\n",
      "|    n_updates        | 9        |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -204     |\n",
      "|    exploration_rate | 0.595    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 390      |\n",
      "|    total_timesteps  | 3200     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.23     |\n",
      "|    n_updates        | 49       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -201     |\n",
      "|    exploration_rate | 0.574    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 408      |\n",
      "|    total_timesteps  | 3360     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.02     |\n",
      "|    n_updates        | 89       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -202     |\n",
      "|    exploration_rate | 0.554    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 425      |\n",
      "|    total_timesteps  | 3520     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.91     |\n",
      "|    n_updates        | 129      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -203     |\n",
      "|    exploration_rate | 0.534    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 444      |\n",
      "|    total_timesteps  | 3680     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.08     |\n",
      "|    n_updates        | 169      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.514    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 462      |\n",
      "|    total_timesteps  | 3840     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.98     |\n",
      "|    n_updates        | 209      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -197     |\n",
      "|    exploration_rate | 0.493    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 481      |\n",
      "|    total_timesteps  | 4000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.75     |\n",
      "|    n_updates        | 249      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -193     |\n",
      "|    exploration_rate | 0.473    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 499      |\n",
      "|    total_timesteps  | 4160     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.01     |\n",
      "|    n_updates        | 289      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -187     |\n",
      "|    exploration_rate | 0.453    |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 516      |\n",
      "|    total_timesteps  | 4320     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.2      |\n",
      "|    n_updates        | 329      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -181     |\n",
      "|    exploration_rate | 0.433    |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 533      |\n",
      "|    total_timesteps  | 4480     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.86     |\n",
      "|    n_updates        | 369      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -174     |\n",
      "|    exploration_rate | 0.412    |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 551      |\n",
      "|    total_timesteps  | 4640     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.86     |\n",
      "|    n_updates        | 409      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -167     |\n",
      "|    exploration_rate | 0.392    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 567      |\n",
      "|    total_timesteps  | 4800     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.47     |\n",
      "|    n_updates        | 449      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -161     |\n",
      "|    exploration_rate | 0.372    |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 584      |\n",
      "|    total_timesteps  | 4960     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.95     |\n",
      "|    n_updates        | 489      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -156     |\n",
      "|    exploration_rate | 0.351    |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 600      |\n",
      "|    total_timesteps  | 5120     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.07     |\n",
      "|    n_updates        | 529      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -154     |\n",
      "|    exploration_rate | 0.331    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 617      |\n",
      "|    total_timesteps  | 5280     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.94     |\n",
      "|    n_updates        | 569      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -147     |\n",
      "|    exploration_rate | 0.311    |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 633      |\n",
      "|    total_timesteps  | 5440     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.72     |\n",
      "|    n_updates        | 609      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -139     |\n",
      "|    exploration_rate | 0.291    |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 650      |\n",
      "|    total_timesteps  | 5600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.38     |\n",
      "|    n_updates        | 649      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -136     |\n",
      "|    exploration_rate | 0.27     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 666      |\n",
      "|    total_timesteps  | 5760     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.13     |\n",
      "|    n_updates        | 689      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -132     |\n",
      "|    exploration_rate | 0.25     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 682      |\n",
      "|    total_timesteps  | 5920     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.16     |\n",
      "|    n_updates        | 729      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -128     |\n",
      "|    exploration_rate | 0.23     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 698      |\n",
      "|    total_timesteps  | 6080     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.19     |\n",
      "|    n_updates        | 769      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -125     |\n",
      "|    exploration_rate | 0.21     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 715      |\n",
      "|    total_timesteps  | 6240     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.33     |\n",
      "|    n_updates        | 809      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -121     |\n",
      "|    exploration_rate | 0.189    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 733      |\n",
      "|    total_timesteps  | 6400     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.45     |\n",
      "|    n_updates        | 849      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -120     |\n",
      "|    exploration_rate | 0.169    |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 750      |\n",
      "|    total_timesteps  | 6560     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.02     |\n",
      "|    n_updates        | 889      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -116     |\n",
      "|    exploration_rate | 0.149    |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 767      |\n",
      "|    total_timesteps  | 6720     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.26     |\n",
      "|    n_updates        | 929      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -112     |\n",
      "|    exploration_rate | 0.129    |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 784      |\n",
      "|    total_timesteps  | 6880     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.72     |\n",
      "|    n_updates        | 969      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -107     |\n",
      "|    exploration_rate | 0.108    |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 801      |\n",
      "|    total_timesteps  | 7040     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.15     |\n",
      "|    n_updates        | 1009     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -106     |\n",
      "|    exploration_rate | 0.088    |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 817      |\n",
      "|    total_timesteps  | 7200     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.53     |\n",
      "|    n_updates        | 1049     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -104     |\n",
      "|    exploration_rate | 0.0677   |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 833      |\n",
      "|    total_timesteps  | 7360     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.42     |\n",
      "|    n_updates        | 1089     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -99.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 849      |\n",
      "|    total_timesteps  | 7520     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.8      |\n",
      "|    n_updates        | 1129     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -95.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 865      |\n",
      "|    total_timesteps  | 7680     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.32     |\n",
      "|    n_updates        | 1169     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -94.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 882      |\n",
      "|    total_timesteps  | 7840     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.993    |\n",
      "|    n_updates        | 1209     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -93.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 898      |\n",
      "|    total_timesteps  | 8000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.78     |\n",
      "|    n_updates        | 1249     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -93.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 915      |\n",
      "|    total_timesteps  | 8160     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.38     |\n",
      "|    n_updates        | 1289     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -94.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 931      |\n",
      "|    total_timesteps  | 8320     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.29     |\n",
      "|    n_updates        | 1329     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -94.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 948      |\n",
      "|    total_timesteps  | 8480     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.86     |\n",
      "|    n_updates        | 1369     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -94.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 964      |\n",
      "|    total_timesteps  | 8640     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.6      |\n",
      "|    n_updates        | 1409     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -95.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 980      |\n",
      "|    total_timesteps  | 8800     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.34     |\n",
      "|    n_updates        | 1449     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -95.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 997      |\n",
      "|    total_timesteps  | 8960     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.69     |\n",
      "|    n_updates        | 1489     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -94.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 1014     |\n",
      "|    total_timesteps  | 9120     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.84     |\n",
      "|    n_updates        | 1529     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -94.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 1031     |\n",
      "|    total_timesteps  | 9280     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.15     |\n",
      "|    n_updates        | 1569     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -94.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1048     |\n",
      "|    total_timesteps  | 9440     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.98     |\n",
      "|    n_updates        | 1609     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -94.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1065     |\n",
      "|    total_timesteps  | 9600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.05     |\n",
      "|    n_updates        | 1649     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -94.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1082     |\n",
      "|    total_timesteps  | 9760     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.19     |\n",
      "|    n_updates        | 1689     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -94.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1099     |\n",
      "|    total_timesteps  | 9920     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.89     |\n",
      "|    n_updates        | 1729     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -94.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1116     |\n",
      "|    total_timesteps  | 10080    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.817    |\n",
      "|    n_updates        | 1769     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -95.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1133     |\n",
      "|    total_timesteps  | 10240    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.78     |\n",
      "|    n_updates        | 1809     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -95.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1150     |\n",
      "|    total_timesteps  | 10400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.13     |\n",
      "|    n_updates        | 1849     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -94.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1166     |\n",
      "|    total_timesteps  | 10560    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.31     |\n",
      "|    n_updates        | 1889     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -93.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1182     |\n",
      "|    total_timesteps  | 10720    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.08     |\n",
      "|    n_updates        | 1929     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -91.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1198     |\n",
      "|    total_timesteps  | 10880    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.68     |\n",
      "|    n_updates        | 1969     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -91.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1215     |\n",
      "|    total_timesteps  | 11040    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.38     |\n",
      "|    n_updates        | 2009     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -91.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1232     |\n",
      "|    total_timesteps  | 11200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.7      |\n",
      "|    n_updates        | 2049     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -90.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1249     |\n",
      "|    total_timesteps  | 11360    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.44     |\n",
      "|    n_updates        | 2089     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -89.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1266     |\n",
      "|    total_timesteps  | 11520    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.62     |\n",
      "|    n_updates        | 2129     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -88      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1283     |\n",
      "|    total_timesteps  | 11680    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.82     |\n",
      "|    n_updates        | 2169     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -87.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1300     |\n",
      "|    total_timesteps  | 11840    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.52     |\n",
      "|    n_updates        | 2209     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -86.5    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1316     |\n",
      "|    total_timesteps  | 12000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.35     |\n",
      "|    n_updates        | 2249     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -86.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1333     |\n",
      "|    total_timesteps  | 12160    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.45     |\n",
      "|    n_updates        | 2289     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -85      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1350     |\n",
      "|    total_timesteps  | 12320    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.43     |\n",
      "|    n_updates        | 2329     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -84.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1367     |\n",
      "|    total_timesteps  | 12480    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.22     |\n",
      "|    n_updates        | 2369     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -85.5    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1384     |\n",
      "|    total_timesteps  | 12640    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.84     |\n",
      "|    n_updates        | 2409     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -86.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1401     |\n",
      "|    total_timesteps  | 12800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.01     |\n",
      "|    n_updates        | 2449     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -86      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1418     |\n",
      "|    total_timesteps  | 12960    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.23     |\n",
      "|    n_updates        | 2489     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -87.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1434     |\n",
      "|    total_timesteps  | 13120    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.79     |\n",
      "|    n_updates        | 2529     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -88.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1452     |\n",
      "|    total_timesteps  | 13280    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.974    |\n",
      "|    n_updates        | 2569     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -89.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1468     |\n",
      "|    total_timesteps  | 13440    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.59     |\n",
      "|    n_updates        | 2609     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -89.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1486     |\n",
      "|    total_timesteps  | 13600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.78     |\n",
      "|    n_updates        | 2649     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -89.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1503     |\n",
      "|    total_timesteps  | 13760    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.07     |\n",
      "|    n_updates        | 2689     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -90.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1521     |\n",
      "|    total_timesteps  | 13920    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.58     |\n",
      "|    n_updates        | 2729     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -92.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1539     |\n",
      "|    total_timesteps  | 14080    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.42     |\n",
      "|    n_updates        | 2769     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -91.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1556     |\n",
      "|    total_timesteps  | 14240    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.98     |\n",
      "|    n_updates        | 2809     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -90.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1572     |\n",
      "|    total_timesteps  | 14400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.69     |\n",
      "|    n_updates        | 2849     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -90.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1588     |\n",
      "|    total_timesteps  | 14560    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.12     |\n",
      "|    n_updates        | 2889     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -90.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1604     |\n",
      "|    total_timesteps  | 14720    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.6      |\n",
      "|    n_updates        | 2929     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | -91.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 9        |\n",
      "|    time_elapsed     | 1620     |\n",
      "|    total_timesteps  | 14880    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.68     |\n",
      "|    n_updates        | 2969     |\n",
      "----------------------------------\n",
      "SB3 DQN training completed in 27.23 minutes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27.23209209839503"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import DQN as SB3DQN\n",
    "\n",
    "\n",
    "def train_sb3_dqn(total_steps=15000):\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    base = os.getcwd()\n",
    "    env = sumo_rl.SumoEnvironment(\n",
    "        net_file=os.path.join(base, \"big-intersection\", \"big-intersection.net.xml\"),\n",
    "        route_file=os.path.join(base, \"big-intersection\", \"routes.rou.xml\"),\n",
    "        single_agent=True,\n",
    "        out_csv_name=\"outputs/big-intersection/sb3-dqn\",\n",
    "        use_gui=False,\n",
    "        num_seconds=600,\n",
    "        delta_time=15,\n",
    "        yellow_time=4,\n",
    "        min_green=5,\n",
    "        max_green=60,\n",
    "    )\n",
    "    model = SB3DQN(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=1e-3,\n",
    "        buffer_size=50000,\n",
    "        train_freq=4,\n",
    "        target_update_interval=1000,\n",
    "        exploration_fraction=0.5,\n",
    "        exploration_final_eps=0.05,\n",
    "        learning_starts=3000,\n",
    "        verbose=1,\n",
    "    )\n",
    "    model.learn(total_timesteps=total_steps)\n",
    "    env.close()\n",
    "    elapsed_min = (time.time() - start) / 60.0\n",
    "    print(f\"SB3 DQN training completed in {elapsed_min:.2f} minutes\")\n",
    "    return elapsed_min\n",
    "\n",
    "train_sb3_dqn(total_steps=15000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>episodes</th>\n",
       "      <th>mean_reward</th>\n",
       "      <th>std_reward</th>\n",
       "      <th>min_reward</th>\n",
       "      <th>max_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>custom_dqn</td>\n",
       "      <td>456</td>\n",
       "      <td>-205.997807</td>\n",
       "      <td>203.186064</td>\n",
       "      <td>-1439.0</td>\n",
       "      <td>-47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sb3_dqn</td>\n",
       "      <td>401</td>\n",
       "      <td>-182.129676</td>\n",
       "      <td>146.575041</td>\n",
       "      <td>-1267.0</td>\n",
       "      <td>-75.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      variant  episodes  mean_reward  std_reward  min_reward  max_reward\n",
       "0  custom_dqn       456  -205.997807  203.186064     -1439.0       -47.0\n",
       "1     sb3_dqn       401  -182.129676  146.575041     -1267.0       -75.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob, os, time\n",
    "\n",
    "def load_episode_reward(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if \"system_total_stopped\" in df.columns:\n",
    "        return -df[\"system_total_stopped\"].iloc[-1]\n",
    "    elif \"reward\" in df.columns:\n",
    "        return df[\"reward\"].sum()\n",
    "    else:\n",
    "        return -df.iloc[-1, -1]\n",
    "\n",
    "def summarize_variant(pattern):\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    if not files:\n",
    "        return None\n",
    "    rewards = [load_episode_reward(f) for f in files]\n",
    "    return {\n",
    "        \"episodes\": len(files),\n",
    "        \"mean_reward\": float(np.mean(rewards)),\n",
    "        \"std_reward\": float(np.std(rewards)),\n",
    "        \"min_reward\": float(np.min(rewards)),\n",
    "        \"max_reward\": float(np.max(rewards)),\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "for name, pattern in [\n",
    "    (\"custom_dqn\", \"outputs/big-intersection/custom-dqn_conn*_ep*.csv\"),\n",
    "    (\"sb3_dqn\", \"outputs/big-intersection/sb3-dqn_conn*_ep*.csv\"),\n",
    "]:\n",
    "    summary = summarize_variant(pattern)\n",
    "    if summary:\n",
    "        summary[\"variant\"] = name\n",
    "        rows.append(summary)\n",
    "\n",
    "pd.DataFrame(rows)[[\"variant\", \"episodes\", \"mean_reward\", \"std_reward\", \"min_reward\", \"max_reward\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Baseline QL:** mean reward = -220.28  28.13  \n",
    "**Optimized QL:** mean reward = -74.16  20.99\n",
    "\n",
    "   ** 3  ** baseline.  :\n",
    "-  learning rate ( decay  0.5  0.05)\n",
    "-   epsilon decay ( 1.0  0.05  3000 )\n",
    "-    \n",
    "\n",
    "Baseline       -   =0.05.         .\n",
    "\n",
    "\n",
    "**Custom DQN:** ~25    \n",
    "**SB3 DQN:** ~55    \n",
    "\n",
    "    (mean reward ~-200 to -180),   DQN  ** 2  **.\n",
    "\n",
    "**Custom DQN :**\n",
    "- Dueling architecture + Double DQN\n",
    "-   reward clipping ([-10, 10])\n",
    "-  overhead  \n",
    "\n",
    "**SB3 DQN:**\n",
    "-    (  )\n",
    "-   \n",
    "-   \n",
    "\n",
    "    ,  SB3   .\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
