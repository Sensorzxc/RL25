{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByKY8Rp-qjuj"
   },
   "source": [
    "# Actor-Critic\n",
    "\n",
    "Теорема о градиенте стратегии связывает градиент целевой функции  и градиент самой стратегии:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi [Q^\\pi(s, a) \\nabla_\\theta \\ln \\pi_\\theta(a \\vert s)]$$\n",
    "\n",
    "Встает вопрос, как оценить $Q^\\pi(s, a)$? В чистом policy-based алгоритме REINFORCE используется отдача $G_t$, полученная методом Монте-Карло в качестве несмещенной оценки $Q^\\pi(s, a)$. В Actor-Critic же предлагается отдельно обучать нейронную сеть Q-функции — критика.\n",
    "\n",
    "Актор-критиком часто называют обобщенный фреймворк (подход), нежели какой-то конкретный алгоритм. Как подход актор-критик не указывает, каким конкретно [policy gradient] методом обучается актор и каким [value based] методом обучается критик. Таким образом актор-критик задает целое [семейство](https://proceedings.neurips.cc/paper_files/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf) различных алгоритмов. Рекомендую в качестве шпаргалки использовать упомянутый в тетрадке с REINFORCE [пост из блога Lilian Weng](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/), посвященный наиболее популярным алгоритмам семейства актор-критиков\n",
    "\n",
    "В данной тетрадке познакомимся с наиболее простым вариантом актор-критика, который так и называют Actor-Critic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "823sXK2eqyte",
    "outputId": "9531e064-5a8f-4e81-9aef-e5000138bfa9"
   },
   "outputs": [],
   "source": [
    "# Cтавим нужные зависимости, если это колаб\n",
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    COLAB = False\n",
    "    pass\n",
    "\n",
    "if COLAB:\n",
    "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
    "    !pip -q install piglet\n",
    "    !pip -q install imageio_ffmpeg\n",
    "    !pip -q install moviepy==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "MRq9NksDrpw7"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8jg4GH8r0zr",
    "outputId": "25f1ffbe-7cd6-4d7c-da74-73110d22c35b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.observation_space=Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "env.action_space=Discrete(2)\n",
      "Action_space: 2 | State_space: (4,)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "\n",
    "print(f'{env.observation_space=}')\n",
    "print(f'{env.action_space=}')\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "print(f'Action_space: {n_actions} | State_space: {env.observation_space.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0udnb6ARBTl"
   },
   "source": [
    "(1 балл)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "2ItOnbBmsDrV"
   },
   "outputs": [],
   "source": [
    "def to_tensor(x, dtype=np.float32):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x\n",
    "    x = np.asarray(x, dtype=dtype)\n",
    "    x = torch.from_numpy(x)\n",
    "    return x\n",
    "\n",
    "def symlog(x):\n",
    "    \"\"\"Compute symlog values for a vector `x`. It's an inverse operation for symexp.\"\"\"\n",
    "    return torch.sign(x) * torch.log(torch.abs(x) + 1)\n",
    "\n",
    "def symexp(x):\n",
    "    \"\"\"Compute symexp values for a vector `x`. It's an inverse operation for symlog.\"\"\"\n",
    "    return torch.sign(x) * (torch.exp(torch.abs(x)) - 1.0)\n",
    "\n",
    "\n",
    "class SymExpModule(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return symexp(x)\n",
    "\n",
    "def select_action_eps_greedy(Q, state, epsilon):\n",
    "    \"\"\"Выбирает действие epsilon-жадно.\"\"\"\n",
    "    if not isinstance(state, torch.Tensor):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "    Q_s = Q(state).detach().numpy()\n",
    "\n",
    "    # action =\n",
    "    ####### Здесь ваш код ########\n",
    "    if np.random.random() < epsilon:\n",
    "        action = np.random.randint(0, len(Q_s))\n",
    "    else:\n",
    "        action = np.argmax(Q_s)\n",
    "    ##############################\n",
    "\n",
    "    action = int(action)\n",
    "    return action\n",
    "\n",
    "def sample_batch(replay_buffer, n_samples):\n",
    "    # sample randomly `n_samples` samples from replay buffer\n",
    "    # and split an array of samples into arrays: states, actions, rewards, next_actions, terminateds\n",
    "    ####### Здесь ваш код ########\n",
    "    rng = np.random.default_rng()\n",
    "    indices = rng.choice(len(replay_buffer), size=n_samples, replace=True)\n",
    "    samples = [replay_buffer[i] for i in indices]\n",
    "    states, actions, rewards, next_states, terminateds = zip(*samples)\n",
    "    ##############################\n",
    "\n",
    "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(terminateds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNMK_gF6sRO-"
   },
   "source": [
    "## Shared-body Actor-Critic\n",
    "\n",
    "Актор и критик могут обучаться в разных режимах — актор только on-policy (шаг обучения на текущей собранной подтраектории), а критик on-policy или off-policy (шаг обучения на текущей подтраектории или на батче из replay buffer). Это с одной стороны привносит гибкость в обучение, с другой — усложняет его.\n",
    "\n",
    "Если актор и критик оба обучаются on-policy, то имеет смысл объединить их сетки в одну и делать общий шаг обратного распространения ошибки. Однако, если они обучаются в разных режимах (и с разной частотой обновления), то велика вероятность, что их шаги обучения могут начать конфликтовать в случае общего тела — для такого варианта намного предпочтительнее разделить их на разные подсети (либо аккуратно настраивать гиперпарметры, чтобы стабилизировать обучение). В целом, рекомендуется использовать общий энкодер наблюдений, а далее как можно скорее разделять головы.\n",
    "\n",
    "Сделаем реализацию актор-критика с общим телом и с on-policy вариантом обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "kvvLaw3Lsd9Y"
   },
   "outputs": [],
   "source": [
    "class ActorBatch:\n",
    "    def __init__(self):\n",
    "        self.logprobs = []\n",
    "        self.q_values = []\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "\n",
    "    def append(self, log_prob, q_value, state=None, action=None):\n",
    "        self.logprobs.append(log_prob)\n",
    "        self.q_values.append(q_value)\n",
    "        if state is not None:\n",
    "            self.states.append(state)\n",
    "        if action is not None:\n",
    "            self.actions.append(action)\n",
    "\n",
    "    def clear(self):\n",
    "        self.logprobs.clear()\n",
    "        self.q_values.clear()\n",
    "        self.states.clear()\n",
    "        self.actions.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoTv-E6rRTfH"
   },
   "source": [
    "(3 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "qtVGcaHUtSCO"
   },
   "outputs": [],
   "source": [
    "class ActorCriticModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Инициализируйте сеть агента с двумя головами: softmax-актора и линейного критика\n",
    "        # self.net, self.actor_head, self.critic_head =\n",
    "        ####### Здесь ваш код ########\n",
    "        # Создаем общее тело (encoder) сети\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = hidden_dim\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        # Actor head: линейный слой для logits действий\n",
    "        self.actor_head = nn.Linear(prev_dim, output_dim)\n",
    "        \n",
    "        # Critic head: линейный слой для Q-значений (одно значение на действие)\n",
    "        self.critic_head = nn.Linear(prev_dim, output_dim)\n",
    "        ##############################\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Вычислите выбранное действие, логарифм вероятности его выбора и соответствующее значение Q-функции\n",
    "        ####### Здесь ваш код ########\n",
    "        # Пропускаем состояние через общее тело\n",
    "        features = self.net(state)\n",
    "        \n",
    "        # Получаем logits для актора и создаем распределение\n",
    "        actor_logits = self.actor_head(features)\n",
    "        dist = Categorical(logits=actor_logits)\n",
    "        \n",
    "        # Сэмплируем действие и получаем log_prob\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        # Получаем Q-значения для всех действий из критика\n",
    "        q_values = self.critic_head(features)\n",
    "        \n",
    "        # Берем Q-значение для выбранного действия\n",
    "        # Обрабатываем случай, когда state - один пример (1D) или батч (2D)\n",
    "        if q_values.dim() == 1:\n",
    "            Q_s_a = q_values[action]\n",
    "        else:\n",
    "            Q_s_a = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        ##############################\n",
    "\n",
    "        return action, log_prob, Q_s_a\n",
    "\n",
    "    def evaluate(self, state):\n",
    "        # Вычислите значения Q-функции для данного состояния\n",
    "        ####### Здесь ваш код ########\n",
    "        # Пропускаем состояние через общее тело\n",
    "        features = self.net(state)\n",
    "        \n",
    "        # Получаем Q-значения для всех действий из критика\n",
    "        q_values = self.critic_head(features)\n",
    "        ##############################\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nco1WpaORXsY"
   },
   "source": [
    "(6 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QhPhsYPBtZPg"
   },
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims, lr, gamma, critic_rb_size):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Инициализируйте модель актор-критика и SGD оптимизатор (например, `torch.optim.Adam)`)\n",
    "        ####### Здесь ваш код ########\n",
    "        self.actor_critic = ActorCriticModel(state_dim, hidden_dims, action_dim)\n",
    "        self.opt = torch.optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "        ##############################\n",
    "\n",
    "        self.actor_batch = ActorBatch()\n",
    "        self.critic_rb = deque(maxlen=critic_rb_size)\n",
    "\n",
    "    def act(self, state):\n",
    "        # Произведите выбор действия и сохраните необходимые данные в батч для последующего обучения\n",
    "        # Не забудьте сделать q_value.detach()\n",
    "        # self.actor_batch.append(..)\n",
    "        ####### Здесь ваш код ########\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "        \n",
    "        # Добавляем batch dimension, если его нет\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        \n",
    "        action, log_prob, q_value = self.actor_critic(state)\n",
    "        \n",
    "                # Сохраняем данные в батч до squeeze (для правильной формы тензоров)    \n",
    "        # Сохраняем также состояние и действие для возможного пересчета Q-значений\n",
    "        # Сохраняем log_prob без detach, чтобы можно было использовать градиенты при необходимости\n",
    "        self.actor_batch.append(\n",
    "            log_prob,\n",
    "            q_value.detach().item(),\n",
    "            state.detach().clone(),\n",
    "            action.detach().clone()\n",
    "        )\n",
    "        \n",
    "        # Убираем batch dimension для возврата\n",
    "        action = action.squeeze(0) if action.dim() > 0 else action\n",
    "        log_prob = log_prob.squeeze(0) if log_prob.dim() > 0 else log_prob\n",
    "        q_value = q_value.squeeze(0) if q_value.dim() > 0 else q_value\n",
    "        \n",
    "        return action.item()\n",
    "        ##############################\n",
    "\n",
    "    def append_to_replay_buffer(self, s, a, r, next_s, terminated):\n",
    "        # Добавьте новый экземпляр данных в память прецедентов.\n",
    "        ####### Здесь ваш код ########\n",
    "        self.critic_rb.append((s, a, r, next_s, terminated))\n",
    "        ##############################\n",
    "\n",
    "    def evaluate(self, state):\n",
    "        return self.actor_critic.evaluate(state)\n",
    "\n",
    "    def update(self, rollout_size, critic_batch_size, critic_updates_per_actor):\n",
    "        if len(self.actor_batch.q_values) < rollout_size:\n",
    "            return\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        loss = self.update_critic(critic_batch_size, critic_updates_per_actor)  \n",
    "        loss = loss + self.update_actor()  # Используем + вместо += для избежания in-place операции\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping для стабильности обучения\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), max_norm=1.0)\n",
    "\n",
    "        self.opt.step()\n",
    "        self.actor_batch.clear()\n",
    "        # Не очищаем critic_rb, так как это replay buffer для off-policy обучения\n",
    "        # self.critic_rb.clear()\n",
    "\n",
    "    def update_actor(self):\n",
    "        # Пересчитываем Q-значения и logprobs после обновления критика для более точной оценки\n",
    "        if len(self.actor_batch.states) > 0 and len(self.actor_batch.actions) > 0:\n",
    "            # Убеждаемся, что состояния имеют правильную форму перед stack\n",
    "            states_list = []\n",
    "            for s in self.actor_batch.states:\n",
    "                # Убираем batch dimension, если он есть\n",
    "                if s.dim() > 1:\n",
    "                    states_list.append(s.squeeze(0))\n",
    "                else:\n",
    "                    states_list.append(s)\n",
    "            states_tensor = torch.stack(states_list)\n",
    "            \n",
    "            actions_tensor = torch.stack(self.actor_batch.actions)\n",
    "            \n",
    "            # Пересчитываем logprobs с градиентами\n",
    "            features = self.actor_critic.net(states_tensor)\n",
    "            actor_logits = self.actor_critic.actor_head(features)\n",
    "            dist = Categorical(logits=actor_logits)\n",
    "            # actions_tensor после stack может иметь форму [batch_size] или [batch_size, 1]\n",
    "            # Преобразуем в одномерный тензор и убеждаемся, что это long\n",
    "            # Преобразуем actions_tensor в одномерный тензор\n",
    "            if actions_tensor.dim() > 1:\n",
    "                actions_flat = actions_tensor.squeeze(-1).long()\n",
    "            else:\n",
    "                actions_flat = actions_tensor.long()\n",
    "            logprobs = dist.log_prob(actions_flat)\n",
    "            \n",
    "            # Получаем Q-значения для всех действий (с градиентами для обновления актора)\n",
    "            q_values_all = self.actor_critic.evaluate(states_tensor)\n",
    "            # Берем Q-значения для выбранных действий\n",
    "            # Убеждаемся, что actions_flat имеет правильную форму [batch_size]\n",
    "            if actions_flat.dim() == 0:\n",
    "                actions_flat = actions_flat.unsqueeze(0)\n",
    "            elif actions_flat.dim() > 1:\n",
    "                actions_flat = actions_flat.squeeze()\n",
    "            \n",
    "            # q_values_all должна иметь форму [batch_size, num_actions]\n",
    "            # actions_flat должна иметь форму [batch_size]\n",
    "            # Для gather нужен индекс формы [batch_size, 1]\n",
    "            # Используем индексацию напрямую, если gather не работает\n",
    "            if q_values_all.dim() == 2 and actions_flat.dim() == 1:\n",
    "                # Стандартный случай: q_values_all [batch_size, num_actions], actions_flat [batch_size]\n",
    "                Q_s_a = q_values_all.gather(1, actions_flat.unsqueeze(1)).squeeze(1)\n",
    "            else:\n",
    "                # Альтернативный способ: используем индексацию\n",
    "                batch_indices = torch.arange(q_values_all.shape[0], device=q_values_all.device)\n",
    "                Q_s_a = q_values_all[batch_indices, actions_flat]\n",
    "            \n",
    "            # Нормализуем Q-значения для стабильности (вычитаем среднее)\n",
    "            Q_s_a = Q_s_a - Q_s_a.mean().detach()\n",
    "        else:\n",
    "            # Fallback: пересчитываем logprobs, если состояния не сохранены\n",
    "            # Используем сохраненные logprobs (они должны иметь градиенты)\n",
    "            logprobs = torch.stack(self.actor_batch.logprobs)\n",
    "            Q_s_a = to_tensor(self.actor_batch.q_values)\n",
    "            # Нормализуем Q-значения для стабильности\n",
    "            Q_s_a = Q_s_a - Q_s_a.mean().detach()\n",
    "\n",
    "        # Реализуйте шаг обновления актора — вычислите ошибку `loss` и произведите шаг обновления градиентным спуском.\n",
    "        ####### Здесь ваш код ########\n",
    "        # Policy gradient loss: -E[Q(s,a) * log π(a|s)]\n",
    "        # Минимизируем отрицательный policy gradient (максимизируем ожидаемую награду)\n",
    "        # Q_s_a уже нормализован выше\n",
    "        loss = -(Q_s_a * logprobs).mean()\n",
    "        ##############################\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def update_critic(self, batch_size, n_updates=1):\n",
    "        # Реализуйте n_updates шагов обучения критика.\n",
    "        ####### Здесь ваш код ########\n",
    "        total_loss = None\n",
    "        n_actual_updates = 0\n",
    "        \n",
    "        for _ in range(n_updates):\n",
    "            if len(self.critic_rb) < batch_size:\n",
    "                break\n",
    "            \n",
    "            # Сэмплируем батч из replay buffer\n",
    "            states, actions, rewards, next_states, terminateds = sample_batch(\n",
    "                self.critic_rb, batch_size\n",
    "            )\n",
    "            \n",
    "            # Вычисляем TD loss\n",
    "            loss = self.compute_td_loss(\n",
    "                states, actions, rewards, next_states, terminateds\n",
    "            )\n",
    "            \n",
    "            if total_loss is None:\n",
    "                total_loss = loss\n",
    "            else:\n",
    "                total_loss = total_loss + loss\n",
    "            n_actual_updates += 1\n",
    "        \n",
    "        # Возвращаем средний loss (или 0, если не было обновлений)\n",
    "        if n_actual_updates > 0:\n",
    "            total_loss = total_loss / n_actual_updates\n",
    "        else:\n",
    "            total_loss = torch.tensor(0.0, requires_grad=True)\n",
    "        \n",
    "        return total_loss\n",
    "        ##############################\n",
    "\n",
    "    def compute_td_loss(\n",
    "        self, states, actions, rewards, next_states, terminateds, regularizer=0.1\n",
    "    ):\n",
    "        # переводим входные данные в тензоры\n",
    "        s = to_tensor(states)                     # shape: [batch_size, state_size]\n",
    "        a = to_tensor(actions, int).long()        # shape: [batch_size]\n",
    "        r = to_tensor(rewards)                    # shape: [batch_size]\n",
    "        s_next = to_tensor(next_states)           # shape: [batch_size, state_size]\n",
    "        term = to_tensor(terminateds, bool)       # shape: [batch_size]\n",
    "\n",
    "\n",
    "        # получаем Q[s, a] для выбранных действий в текущих состояниях (для каждого примера из батча)\n",
    "        # Q_s_a = ...\n",
    "        ####### Здесь ваш код ########\n",
    "        q_values = self.actor_critic.evaluate(s)\n",
    "        Q_s_a = q_values.gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "        ##############################\n",
    "\n",
    "        # получаем Q[s_next, *] — значения полезности всех действий в следующих состояниях\n",
    "        # Q_sn = ...,\n",
    "        # а затем вычисляем V*[s_next] — оптимальные значения полезности следующих состояний\n",
    "        # V_sn = ...\n",
    "        ####### Здесь ваш код ########\n",
    "        with torch.no_grad():\n",
    "            Q_sn = self.actor_critic.evaluate(s_next)\n",
    "            # Для Actor-Critic используем значение из текущей политики, а не max\n",
    "            # Получаем вероятности действий из актора\n",
    "            features_next = self.actor_critic.net(s_next)\n",
    "            actor_logits_next = self.actor_critic.actor_head(features_next)\n",
    "            probs_next = torch.softmax(actor_logits_next, dim=1)\n",
    "            # V(s_next) = E_a~π[Q(s_next, a)] = sum_a π(a|s_next) * Q(s_next, a)\n",
    "            V_sn = (probs_next * Q_sn).sum(dim=1)\n",
    "        ##############################\n",
    "\n",
    "        # вычисляем TD target и далее TD error\n",
    "        # target = ...\n",
    "        # td_error = ...\n",
    "        ####### Здесь ваш код ########\n",
    "        # TD target: r + gamma * V*(s_next) * (1 - terminated)\n",
    "        target = r + self.gamma * V_sn * (~term).float()\n",
    "        # TD error: Q(s,a) - target\n",
    "        td_error = Q_s_a - target\n",
    "        ##############################\n",
    "\n",
    "        # MSE loss для минимизации\n",
    "        loss = torch.mean(td_error ** 2)\n",
    "        # добавляем регуляризацию на значения Q\n",
    "        loss += regularizer * Q_s_a.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "jAID76g8tg_R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=2007 | avg_return=10.000\n",
      "global_step=4005 | avg_return=11.500\n",
      "global_step=6010 | avg_return=21.333\n",
      "global_step=8006 | avg_return=19.250\n",
      "global_step=10019 | avg_return=20.600\n",
      "global_step=12011 | avg_return=20.333\n",
      "global_step=14035 | avg_return=23.000\n",
      "global_step=16024 | avg_return=24.000\n",
      "global_step=18012 | avg_return=29.778\n",
      "global_step=20052 | avg_return=36.400\n",
      "global_step=22111 | avg_return=48.200\n",
      "global_step=24096 | avg_return=67.300\n",
      "global_step=26047 | avg_return=76.400\n",
      "global_step=28029 | avg_return=90.200\n",
      "global_step=30110 | avg_return=101.100\n",
      "global_step=32153 | avg_return=118.300\n",
      "global_step=34102 | avg_return=126.000\n",
      "global_step=36086 | avg_return=135.900\n",
      "global_step=38065 | avg_return=138.900\n",
      "global_step=40123 | avg_return=144.300\n",
      "global_step=42047 | avg_return=148.000\n",
      "global_step=44089 | avg_return=143.800\n",
      "global_step=46187 | avg_return=154.000\n",
      "global_step=48001 | avg_return=152.400\n",
      "global_step=50113 | avg_return=152.400\n",
      "global_step=52081 | avg_return=146.800\n",
      "global_step=54068 | avg_return=153.900\n",
      "global_step=56134 | avg_return=154.800\n",
      "global_step=58111 | avg_return=160.600\n",
      "global_step=60090 | avg_return=169.900\n",
      "global_step=62167 | avg_return=170.300\n",
      "global_step=64013 | avg_return=171.900\n",
      "global_step=66033 | avg_return=159.600\n",
      "global_step=68113 | avg_return=166.500\n",
      "global_step=70078 | avg_return=173.000\n",
      "global_step=72006 | avg_return=160.500\n",
      "global_step=74031 | avg_return=152.300\n",
      "global_step=76006 | avg_return=139.300\n",
      "global_step=78010 | avg_return=132.500\n",
      "global_step=80090 | avg_return=117.600\n",
      "global_step=82105 | avg_return=127.500\n",
      "global_step=84014 | avg_return=123.900\n",
      "global_step=86072 | avg_return=128.800\n",
      "global_step=88106 | avg_return=122.300\n",
      "global_step=90073 | avg_return=117.700\n",
      "global_step=92048 | avg_return=128.700\n",
      "global_step=94094 | avg_return=136.600\n",
      "global_step=96073 | avg_return=148.400\n",
      "global_step=98051 | avg_return=152.100\n"
     ]
    }
   ],
   "source": [
    "def run_actor_critic(\n",
    "        env_name=\"CartPole-v1\",\n",
    "        hidden_dims=(128, 128), lr=5e-4,\n",
    "        total_max_steps=200_000,\n",
    "        train_schedule=16, replay_buffer_size=50000, batch_size=64, critic_updates_per_actor=4,\n",
    "        eval_schedule=1000, smooth_ret_window=10, success_ret=200.\n",
    "):\n",
    "    env = gym.make(env_name)\n",
    "    episode_return_history = deque(maxlen=smooth_ret_window)\n",
    "\n",
    "    agent = ActorCriticAgent(\n",
    "        state_dim=env.observation_space.shape[0], action_dim=env.action_space.n, hidden_dims=hidden_dims,\n",
    "        lr=lr, gamma=.995, critic_rb_size=replay_buffer_size\n",
    "    )\n",
    "\n",
    "    s, _ = env.reset()\n",
    "    done, episode_return = False, 0.\n",
    "    eval = False\n",
    "\n",
    "    for global_step in range(1, total_max_steps+1):\n",
    "        a = agent.act(s)\n",
    "        s_next, r, terminated, truncated, _ = env.step(a)\n",
    "        episode_return += r\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # train step\n",
    "        agent.append_to_replay_buffer(s, a, r, s_next, terminated)\n",
    "        agent.update(train_schedule, batch_size, critic_updates_per_actor)\n",
    "\n",
    "        # evaluate\n",
    "        if global_step % eval_schedule == 0:\n",
    "            eval = True\n",
    "\n",
    "        s = s_next\n",
    "        if done:\n",
    "            if eval:\n",
    "                episode_return_history.append(episode_return)\n",
    "                avg_return = np.mean(episode_return_history)\n",
    "                print(f'{global_step=} | {avg_return=:.3f}')\n",
    "                if avg_return >= success_ret:\n",
    "                    print('Решено!')\n",
    "                    break\n",
    "\n",
    "            s, _ = env.reset()\n",
    "            done, episode_return = False, 0.\n",
    "            eval = False\n",
    "\n",
    "run_actor_critic(eval_schedule=2000, total_max_steps=100_000)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
